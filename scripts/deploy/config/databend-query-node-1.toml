# Usage:
# databend-query -c databend_query_config_spec.toml

[query]
max_active_sessions = 256
shutdown_wait_timeout_ms = 5000

# For flight rpc.
flight_api_address = "0.0.0.0:9091"

# Databend Query http address.
# For admin RESET API.
admin_api_address = "0.0.0.0:8080"

# Databend Query metrics RESET API.
metric_api_address = "0.0.0.0:7070"
discovery_address = "localhost:8001"
# Databend Query MySQL Handler.
mysql_handler_host = "0.0.0.0"
mysql_handler_port = 3307

# Databend Query ClickHouse Handler.
clickhouse_http_handler_host = "0.0.0.0"
clickhouse_http_handler_port = 8124

# Databend Query HTTP Handler.
http_handler_host = "0.0.0.0"
http_handler_port = 8001

# Databend Query FlightSQL Handler.
flight_sql_handler_host = "0.0.0.0"
flight_sql_handler_port = 8900

tenant_id = "test_tenant"
cluster_id = "test_cluster"

table_engine_memory_enabled = true
default_storage_format = 'parquet'
default_compression = 'zstd'

enable_udf_server = true
udf_server_allow_list = ['http://0.0.0.0:8815']
cloud_control_grpc_server_address = "http://0.0.0.0:50051"

[[query.users]]
name = "root"
auth_type = "no_password"

[[query.users]]
name = "default"
auth_type = "no_password"

 [[query.users]]
 name = "databend"
 auth_type = "double_sha1_password"
 # echo -n "databend" | sha1sum | cut -d' ' -f1 | xxd -r -p | sha1sum
 auth_string = "3081f32caef285c232d066033c89a78d88a6d8a5"

# This for test
[[query.udfs]]
name = "ping"
definition = "CREATE FUNCTION ping(STRING) RETURNS STRING LANGUAGE python HANDLER = 'ping' ADDRESS = 'http://0.0.0.0:8815'"

[query.settings]
aggregate_spilling_memory_ratio = 60
join_spilling_memory_ratio = 60

[log]

[log.file]
level = "DEBUG"
format = "text"
dir = "./.databend/logs_1"

[log.query]
on = true

[log.structlog]
on = true
dir = "./.databend/structlog_1"

[meta]
# It is a list of `grpc_api_advertise_host:<grpc-api-port>` of databend-meta config
endpoints = ["0.0.0.0:9191"]
username = "root"
password = "root"
client_timeout_in_second = 60
auto_sync_interval = 60

# Storage config.
[storage]
# fs | s3 | azblob | obs | oss
type = "fs"

# Set a local folder to store your data.
# Comment out this block if you're NOT using local file system as storage.
[storage.fs]
data_path = "./.databend/stateless_test_data"

# Cache config.
[cache]
# Type of storage to keep the table data cache
#
# available options: [none|disk]
# default is "none", which disable table data cache
# use "disk" to enabled disk cache
data_cache_storage = "none"

[cache.disk]
# cache path
path = "./.databend/_cache"
# max bytes of cached data 20G
max_bytes = 21474836480
